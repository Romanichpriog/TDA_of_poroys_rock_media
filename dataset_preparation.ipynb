{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9333ac7",
   "metadata": {},
   "source": [
    "# Download script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c316c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "base_url = 'https://www.digitalrocksportal.org/projects/372'\n",
    "download_directory = 'downloads'\n",
    "\n",
    "os.makedirs(download_directory, exist_ok=True)\n",
    "\n",
    "def download_file(url, directory, filename):\n",
    "    if not filename:\n",
    "        parsed_url = urlparse(url)\n",
    "        filename = os.path.basename(parsed_url.path)\n",
    "\n",
    "    if not filename:\n",
    "        filename = 'downloaded_file'\n",
    "\n",
    "    local_filename = os.path.join(directory, filename)\n",
    "\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    return local_filename\n",
    "\n",
    "def fetch_links(url, keyword):\n",
    "    \"\"\"Функция для извлечения ссылок по ключевому слову\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = [\n",
    "        urljoin(url, a['href']) for a in soup.find_all('a', href=True)\n",
    "        if keyword in a['href']\n",
    "    ]\n",
    "    return links\n",
    "\n",
    "\n",
    "def process_file_links(file_links_soup, directory, name):\n",
    "    for div in file_links_soup.find_all('div', class_='dataset-file'):\n",
    "        h4 = div.find('h4')\n",
    "        if h4 and  h4['title'][0].isdigit():\n",
    "            title = h4['title']\n",
    "        else:\n",
    "            title = name+\"_\"+h4['title']\n",
    "        print(h4['title'])\n",
    "\n",
    "        download_link = [a['href'] for a in div.find_all('a', href=True) if \"download\" in a['href'] and \"downloadMeta\" not in a['href']]\n",
    "        print(download_link[0])\n",
    "        if download_link[0]:\n",
    "            file_url = urljoin(base_url, download_link[0])\n",
    "            print(f\"Скачиваю файл: {file_url} как {title or 'default'}\")\n",
    "            download_file(file_url, directory, title)\n",
    "\n",
    "\n",
    "def process_analysis_data(analysis_data_url):\n",
    "    print(f\"Обрабатываю origin_data: {analysis_data_url}\")\n",
    "\n",
    "\n",
    "    response = requests.get(analysis_data_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    names = [[j for j in i.get_text(strip=False).split() if j != \"\\n\" and j[0].isdigit()][0] for i in soup.find_all('h2')]\n",
    "    print(names[0])\n",
    "\n",
    "    process_file_links(soup, download_directory, names[0])\n",
    "\n",
    "\n",
    "\n",
    "def process_origin_data(origin_data_url):\n",
    "    print(f\"Обрабатываю origin_data: {origin_data_url}\")\n",
    "\n",
    "\n",
    "    response = requests.get(origin_data_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    names = [[j for j in i.get_text(strip=False).split() if j != \"\\n\" and j[0].isdigit()][0] for i in soup.find_all('h2')]\n",
    "    print(names[0])\n",
    "\n",
    "    process_file_links(soup, download_directory, names[0])\n",
    "\n",
    "    analysis_data_links = fetch_links(origin_data_url, 'analysis_data')\n",
    "\n",
    "    for analysis_data_url in analysis_data_links:\n",
    "        process_analysis_data(analysis_data_url)\n",
    "\n",
    "def process_sample(sample_url):\n",
    "    print(f\"Обрабатываю сэмпл: {sample_url}\")\n",
    "\n",
    "\n",
    "    origin_data_links = fetch_links(sample_url, 'origin_data')\n",
    "\n",
    "    for origin_data_url in origin_data_links:\n",
    "        process_origin_data(origin_data_url)\n",
    "\n",
    "def main():\n",
    "\n",
    "    sample_links = fetch_links(base_url, 'sample')\n",
    "\n",
    "\n",
    "    for sample_url in sample_links:\n",
    "        process_sample(sample_url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05197b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import random\n",
    "import struct\n",
    "import time\n",
    "from array import array\n",
    "from datetime import timedelta\n",
    "from os.path import join\n",
    "\n",
    "import cv2\n",
    "import gudhi as gd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# from src.persistence import sublevel_persistence\n",
    "# from src.models import Transformer, ConvolutionalPersistenceHomologyTransformer, DirectionalPersistenceHomologyTransformer\n",
    "import pandas as pd\n",
    "import porespy as ps\n",
    "import scipy as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "from torchvision.transforms.v2 import Compose, Lambda\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pickle\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import quantimpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.ndimage import distance_transform_edt, grey_dilation\n",
    "from torchvision.transforms.v2 import Transform\n",
    "import os\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from hdf5storage import loadmat\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as kb\n",
    "from scipy.ndimage import distance_transform_edt as distance\n",
    "from tensorflow.keras.layers import Conv2D, Input, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "np.set_printoptions(precision=3, linewidth=120, edgeitems=45, threshold=100)\n",
    "torch.set_printoptions(precision=3, linewidth=120, edgeitems=45, threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getName(filename):\n",
    "    dl = 10\n",
    "    while filename[dl:dl +3] != \"256\" and filename[dl:dl +3] != \"480\":\n",
    "        dl += 1\n",
    "    return filename[:dl +3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diagramm(image):\n",
    "    distances = distance_transform_edt(image == 0) - distance_transform_edt(image == 1)\n",
    "    comp = gd.CubicalComplex(top_dimensional_cells = distances)\n",
    "    return comp.persistence(homology_coeff_field=19, min_persistence=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf55a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "file_names = []\n",
    "distances = []\n",
    "mat_datas = []\n",
    "fils = glob.glob(\"downloads/*\")\n",
    "fils.sort()\n",
    "i = 0\n",
    "\n",
    "while i < len(fils):\n",
    "    has_y = False\n",
    "    has_data = False\n",
    "    has_pkl = False\n",
    "    name0 = getName(fils[i])\n",
    "    while i < len(fils) and getName(fils[i]) == name0:\n",
    "        if fils[i].endswith(\"_P_1_MPa.csv\"):\n",
    "            df = pd.read_csv(fils[i], sep=\",\", header=None)\n",
    "            has_y = True\n",
    "        if fils[i].endswith(\".pkl\"):\n",
    "            has_pkl = True\n",
    "            fname = fils[i]\n",
    "            \n",
    "        if getName(fils[i]) + \".mat\" == fils[i] and getName(fils[i])[-3:] == '256':\n",
    "            print(getName(fils[i]) + \".mat\")\n",
    "            try:\n",
    "                mat_data = loadmat(fils[i])[\"bin\"]\n",
    "                dist = -distance_transform_edt(mat_data==1) + distance_transform_edt(mat_data==0)\n",
    "                has_data = True\n",
    "            except KeyError:\n",
    "                has_data = False\n",
    "\n",
    "\n",
    "        i += 1\n",
    "    if (has_pkl and has_y and has_data): #and has_data\n",
    "        y.append(df.iloc[4].values[-1])\n",
    "        file_names.append(fname)\n",
    "        distances.append(dist)\n",
    "        mat_datas.append(mat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mat_datas, open(\"./natures_images\", \"wb\"))\n",
    "pickle.dump(y, open(\"./y_natutes\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1985ba2",
   "metadata": {},
   "source": [
    "# Calculate curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_images_dist = np.array(distances)\n",
    "thresholds = np.linspace(np_images_dist.min(), np_images_dist.max(), num=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cb83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def calculate_betti(filename, thresholds):\n",
    "      with open(filename, \"rb\") as f:\n",
    "        persistence = pickle.load(f)\n",
    "\n",
    "        all_events = []\n",
    "\n",
    "        dots = [[], [], []]\n",
    "\n",
    "        for item in persistence:\n",
    "            all_events += [(item[1][0], +1, item[0])]\n",
    "            all_events += [(item[1][1], -1, item[0])]\n",
    "            dots[item[0]].append(item[1])\n",
    "\n",
    "        all_events.sort()\n",
    "\n",
    "        betti = []\n",
    "        betti_current = [0, 0, 0]\n",
    "\n",
    "        threshold_index = 0\n",
    "\n",
    "        for event in all_events:\n",
    "            t, delta, dim = event\n",
    "            while threshold_index < len(thresholds) and t >= thresholds[threshold_index]:\n",
    "                betti.append(betti_current[:])\n",
    "                threshold_index += 1\n",
    "            betti_current[int(dim)] += delta\n",
    "        return (betti, dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "betti_curves = []\n",
    "for file in file_names:\n",
    "    betti_curves.append(calculate_betti(file, thresholds)[0])\n",
    "    dots = calculate_betti(file, thresholds)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f30544",
   "metadata": {},
   "outputs": [],
   "source": [
    "minkowski_curves = []\n",
    "from quantimpy import minkowski\n",
    "\n",
    "for dis in distances:\n",
    "    curve = []\n",
    "    for thr in thresholds:\n",
    "        curve.append(minkowski.functionals(np.ascontiguousarray(dis < thr)))\n",
    "    minkowski_curves.append(curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834117d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.dstack([np.array(minkowski_curves), np.array(betti_curves)])\n",
    "target = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcac9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(features, open(\"./natures_minkovsky+betti\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1ab80c",
   "metadata": {},
   "source": [
    "# calculating Diagrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b187a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction_filter3d(img, alpha, beta):\n",
    "    width, height, depth = img.shape\n",
    "    \n",
    "    # Центрирование по каждой оси (для координат относительно центра)\n",
    "    X = (np.cos(alpha)*np.sin(beta) - (np.arange(width)  - (width/2  - 0.5)) / (width  * np.sqrt(3))) * np.cos(alpha)*np.sin(beta) / 2\n",
    "    Y = (np.sin(alpha)*np.sin(beta) - (np.arange(height) - (height/2 - 0.5)) / (height * np.sqrt(3))) * np.sin(alpha)*np.sin(beta) / 2\n",
    "    Z = (np.cos(beta)              - (np.arange(depth)  - (depth/2  - 0.5)) / (depth  * np.sqrt(3))) * np.cos(beta)              / 2\n",
    "    \n",
    "    # Складываем, как в 2D варианте!\n",
    "    direction_filter = (\n",
    "        X.reshape(-1, 1, 1) +  # вес по оси X повторяется для всех Y,Z\n",
    "        Y.reshape(1, -1, 1) +  # вес по оси Y повторяется для всех X,Z\n",
    "        Z.reshape(1, 1, -1)    # вес по оси Z повторяется для всех X,Y\n",
    "    )\n",
    "    return torch.Tensor(np.multiply(direction_filter, img))\n",
    "\n",
    "\n",
    "\n",
    "class Direction(Transform):\n",
    "    \"\"\"Transform an image with the direction transform.\"\"\"\n",
    "    def __init__(self, alphas):\n",
    "        super().__init__()\n",
    "        self.alphas = alphas\n",
    "        self.weight = alphas\n",
    "\n",
    "    def transform3d(self, inpt: Any, params: Dict[str, Any]) -> Any:\n",
    "        output = torch.zeros(len(self.alphas), inpt.shape[-3], inpt.shape[-2], inpt.shape[-1])\n",
    "        for i, alpha in enumerate(self.alphas):\n",
    "            output[i] = direction_filter3d(inpt, alpha[0], alpha[1])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02607ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pht(image, pos=None, eps=None):\n",
    "\n",
    "    if len(image.shape) != 4:\n",
    "        raise ValueError(\"Image tensor dimensions should be (channel, width, height).\")\n",
    "\n",
    "    dim = 6 if pos is not None else 3\n",
    "    dgm_pht = torch.zeros((0, dim))\n",
    "    dgms = sublevel_persistence(image, eps=eps, pos=pos, sort=\"persistence\")\n",
    "\n",
    "    for i, dgm in enumerate(dgms):\n",
    "        dgm_pht = torch.cat([dgm_pht, dgm])\n",
    "\n",
    "    return dgm_pht\n",
    "\n",
    "\n",
    "def sublevel_persistence(image, eps=None, pos=None, inf=\"max\", sort=\"birth\"):\n",
    "\n",
    "    if len(image.shape) != 4:\n",
    "        raise ValueError(\"Image tensor dimensions should be (channel, width, height).\")\n",
    "\n",
    "    diagrams = []\n",
    "    persistence = lambda x: x[:, 1] - x[:, 0]\n",
    "\n",
    "    # for each channel\n",
    "    for k, channel in enumerate(image):\n",
    "        diagram_channel_gudhi = gd.CubicalComplex(\n",
    "            top_dimensional_cells=channel\n",
    "        ).persistence()\n",
    "\n",
    "        # convert a diagram from GUDHI format to n x 3 ndarray\n",
    "        diagram_channel = np.zeros((len(diagram_channel_gudhi), 3))\n",
    "        for i, (dim, (birth, death)) in enumerate(diagram_channel_gudhi):\n",
    "            diagram_channel[i] = (birth, death, dim)\n",
    "\n",
    "        # work with infs\n",
    "        if inf == \"max\":\n",
    "            diagram_channel = np.nan_to_num(diagram_channel, posinf=torch.max(channel))\n",
    "        elif inf == \"remove\":\n",
    "            diagram_channel = diagram_channel[~np.isinf(diagram_channel).any(axis=1)]\n",
    "        else:\n",
    "            raise ValueError(\"Inf should be 'max' or 'remove'.\")\n",
    "\n",
    "        # remove points w/ persistence less \\eps\n",
    "        if eps is not None:\n",
    "            diagram_channel = diagram_channel[persistence(diagram_channel) > eps]\n",
    "\n",
    "        # add positional encoding\n",
    "        if pos is not None:\n",
    "            pos_elements1 = np.repeat(pos[k][0], len(diagram_channel))[..., np.newaxis]\n",
    "            pos_elements2 = np.repeat(pos[k][1], len(diagram_channel))[..., np.newaxis]\n",
    "            pos_idx = np.repeat(k, len(diagram_channel))[..., np.newaxis]\n",
    "            diagram_channel = np.concatenate(\n",
    "                [diagram_channel, pos_elements1, pos_elements2, pos_idx], axis=1\n",
    "            )\n",
    "\n",
    "        # sort by dim, then birth or persistence\n",
    "        if sort == \"birth\":\n",
    "            sort_idx = np.lexsort([diagram_channel[:, 0], diagram_channel[:, 2]])\n",
    "        elif sort == \"persistence\":\n",
    "            sort_idx = np.lexsort([persistence(diagram_channel), diagram_channel[:, 2]])\n",
    "        else:\n",
    "            raise ValueError(\"Sort should be 'birth' or 'persistence'.\")\n",
    "        diagram_channel = diagram_channel[sort_idx]\n",
    "\n",
    "        diagrams.append(torch.tensor(diagram_channel))\n",
    "\n",
    "    return diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cb8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6bdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from torchvision.transforms.v2 import Compose, ToDtype\n",
    "\n",
    "transform_rock_image = Compose(\n",
    "    [\n",
    "        ToDtype(torch.float32),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "alphas = np.linspace(0, 360, 6, endpoint=True)\n",
    "betas = np.linspace(0, 180, 3, endpoint=True)\n",
    "pairs_deg = np.stack([np.repeat(alphas, 3), np.tile(betas, 6)], axis=1)\n",
    "pairs_deg = pairs_deg / 180.0 * np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = Direction(pairs_deg)\n",
    "\n",
    "\n",
    "def get_diagram_filtered(image):\n",
    "    image_sedt = -distance_transform_edt(image == 0) + distance_transform_edt(image == 1)\n",
    "    ingt = torch.from_numpy(image_sedt)\n",
    "    imgt = transform_rock_image(ingt)\n",
    "    image_filtered = func.transform3d(imgt, [])\n",
    "    imgt = imgt[None]\n",
    "    diagram = pht(image_filtered, pos=pairs_deg, eps=0.05)\n",
    "    diagram_filterd = pht(imgt, eps=0.05)\n",
    "    return diagram, diagram_filterd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dfaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "images = pickle.load(open(\"/home/jupyter/datasphere/project/natures_images\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cd081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def process_image(im):\n",
    "    diagram, diagram_filterd = get_diagram_filtered(im)\n",
    "    return diagram, diagram_filterd\n",
    "\n",
    "\n",
    "diagrams_filtered = []\n",
    "diagrams = []\n",
    "\n",
    "with Pool(processes=16) as pool:   \n",
    "    results = list(tqdm(\n",
    "        pool.imap(process_image, images),\n",
    "        total=len(images)\n",
    "    ))\n",
    "\n",
    "diagrams_filtered, diagrams = zip(*results)\n",
    "diagrams_filtered = list(diagrams_filtered)\n",
    "diagrams = list(diagrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d27735",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'./natures_diagrams_filtered.pkl'\n",
    "pickle.dump(diagrams_filtered, open(filename, \"wb\"))\n",
    "filename = f'./diagram_natures.pkl'\n",
    "pickle.dump(diagrams, open(filename, \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dip_top",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
